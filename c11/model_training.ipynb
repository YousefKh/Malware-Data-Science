{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developed-hobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 525,313\n",
      "Trainable params: 525,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import mmh3\n",
    "import re\n",
    "import os\n",
    "from model_arch import my_model\n",
    "\n",
    "def read_file(sha, dir):\n",
    "    with open(os.path.join(dir, sha), 'rb') as fp:\n",
    "        file = fp.read().decode(errors = 'replace')\n",
    "    return file\n",
    "def extract_features(sha, path_to_files_dir, hash_dim= 1024, split_regex = r\"\\s+\"):\n",
    "    file = read_file(sha = sha, dir = path_to_files_dir)\n",
    "    tokens = re.split(pattern= split_regex, string= file)\n",
    "    # take the modulo (hash of each token) so that each token is replaced by bucket(catergory) from 1:hash_dim\n",
    "    token_hash_buckets = [\n",
    "        (mmh3.hash(w) % (hash_dim - 1) + 1) for w in tokens]\n",
    "    # Finally, we'll count how many hits each bucket got, so that our features\n",
    "    # always have length hash_dim, regardless of the size of the HTML file:\n",
    "    token_bucket_counts = np.zeros(hash_dim)\n",
    "    # this returns the frequency counts for each unique value in token_hash_buckets:\n",
    "    buckets, counts = np.unique(token_hash_buckets, return_counts= True)\n",
    "    # and now we insert these counts into our token_bucket_counts object:\n",
    "    for bucket, count in zip(buckets, counts):\n",
    "        token_bucket_counts[bucket] = count\n",
    "    return np.array(token_bucket_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-resistance",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deluxe-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gen(benign_files, malicious_files, path_to_benign_files,\n",
    "           path_to_malicious_files, batch_size, features_length=1024):\n",
    "    n_samples_per_class = batch_size // 2\n",
    "    assert len(benign_files) >= n_samples_per_class\n",
    "    assert len(malicious_files) >= n_samples_per_class\n",
    "    while True:\n",
    "        ben_features = [\n",
    "            extract_features(sha,path_to_files_dir = path_to_benign_files, hash_dim = features_length)\n",
    "            for sha in np.random.choice(benign_files, n_samples_per_class,replace= False)]\n",
    "        \n",
    "        mal_features = [\n",
    "            extract_features(sha, path_to_files_dir= path_to_malicious_files, hash_dim = features_length)\n",
    "            for sha in np.random.choice(malicious_files, n_samples_per_class, replace= False)]\n",
    "        \n",
    "        all_features = ben_features + mal_features\n",
    "        labels = [0 for i in range(n_samples_per_class)] + [1 for i in range(n_samples_per_class)]\n",
    "        idx = np.random.choice(range(batch_size), batch_size)\n",
    "        all_features = np.array([np.array(all_features[i]) for i in idx])\n",
    "        labels = np.array([labels[i] for i in idx])\n",
    "        yield all_features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-powder",
   "metadata": {},
   "source": [
    "# Training Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "celtic-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_gen(features_length, batch_size):\n",
    "    path_to_training_benign_files = 'html/benign_files/training/'\n",
    "    path_to_training_malicious_files = 'html/malicious_files/training/'\n",
    "\n",
    "\n",
    "    train_benign_files = os.listdir(path_to_training_benign_files)\n",
    "    train_malicious_files = os.listdir(path_to_training_malicious_files)\n",
    "\n",
    "    # Make our training data generator\n",
    "    training_gen = my_gen(benign_files = train_benign_files, malicious_files = train_malicious_files,\n",
    "                          path_to_benign_files = path_to_training_benign_files,\n",
    "                          path_to_malicious_files = path_to_training_malicious_files,\n",
    "                          batch_size= batch_size, features_length= features_length)\n",
    "    return training_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-senate",
   "metadata": {},
   "source": [
    "# Validation Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indie-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_data(features_length, n_validation_files):\n",
    "\n",
    "    # Read path files\n",
    "    path_to_validation_benign_files = 'html/benign_files/validation/'\n",
    "    path_to_validation_malicious_files = 'html/benign_files/validation/'\n",
    "\n",
    "    # Get the validation keys:\n",
    "    val_benign_file_keys = os.listdir(path_to_validation_benign_files)\n",
    "    val_malicious_file_keys = os.listdir(path_to_validation_malicious_files)\n",
    "\n",
    "    # Grab the validation data and extract the features:\n",
    "    validation_data = next(my_gen(\n",
    "        benign_files = val_benign_file_keys,\n",
    "        malicious_files = val_malicious_file_keys,\n",
    "        path_to_benign_files = path_to_validation_benign_files,\n",
    "        path_to_malicious_files = path_to_validation_malicious_files,\n",
    "        batch_size = 10000,\n",
    "        features_length = features_length\n",
    "    ))\n",
    "    \n",
    "    return validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-still",
   "metadata": {},
   "source": [
    "# Try model with Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "material-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39/39 [==============================] - 25s 610ms/step - loss: 0.4665 - accuracy: 0.7793\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 24s 622ms/step - loss: 0.2143 - accuracy: 0.9141\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 24s 627ms/step - loss: 0.1934 - accuracy: 0.9245\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 25s 638ms/step - loss: 0.1667 - accuracy: 0.9311\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 23s 589ms/step - loss: 0.1605 - accuracy: 0.9367\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 24s 604ms/step - loss: 0.1594 - accuracy: 0.9346\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 24s 613ms/step - loss: 0.1426 - accuracy: 0.9405\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 24s 600ms/step - loss: 0.1185 - accuracy: 0.9503\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 23s 601ms/step - loss: 0.1134 - accuracy: 0.9607\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 24s 613ms/step - loss: 0.1190 - accuracy: 0.9547\n",
      "Epoch 1/10\n",
      "39/39 [==============================] - 25s 653ms/step - loss: 0.1135 - accuracy: 0.9561 - val_loss: 1.9892 - val_accuracy: 0.4997\n",
      "Epoch 2/10\n",
      "39/39 [==============================] - 24s 628ms/step - loss: 0.1259 - accuracy: 0.9527 - val_loss: 1.6654 - val_accuracy: 0.4948\n",
      "Epoch 3/10\n",
      "39/39 [==============================] - 24s 632ms/step - loss: 0.1185 - accuracy: 0.9578 - val_loss: 1.7273 - val_accuracy: 0.4951\n",
      "Epoch 4/10\n",
      "39/39 [==============================] - 24s 641ms/step - loss: 0.1185 - accuracy: 0.9574 - val_loss: 1.8494 - val_accuracy: 0.4996\n",
      "Epoch 5/10\n",
      "39/39 [==============================] - 24s 635ms/step - loss: 0.1087 - accuracy: 0.9568 - val_loss: 1.8445 - val_accuracy: 0.4957\n",
      "Epoch 6/10\n",
      "39/39 [==============================] - 24s 630ms/step - loss: 0.1069 - accuracy: 0.9611 - val_loss: 1.9775 - val_accuracy: 0.5005\n",
      "Epoch 7/10\n",
      "39/39 [==============================] - 24s 623ms/step - loss: 0.1148 - accuracy: 0.9555 - val_loss: 2.1587 - val_accuracy: 0.4997\n",
      "Epoch 8/10\n",
      "39/39 [==============================] - 24s 637ms/step - loss: 0.1074 - accuracy: 0.9615 - val_loss: 1.9990 - val_accuracy: 0.4978\n",
      "Epoch 9/10\n",
      "39/39 [==============================] - 24s 630ms/step - loss: 0.1055 - accuracy: 0.9629 - val_loss: 1.9341 - val_accuracy: 0.5027\n",
      "Epoch 10/10\n",
      "39/39 [==============================] - 24s 620ms/step - loss: 0.0952 - accuracy: 0.9664 - val_loss: 1.9662 - val_accuracy: 0.4951\n"
     ]
    }
   ],
   "source": [
    "def model_with_val(model, training_gen,\n",
    "                                      steps_per_epoch,\n",
    "                                      features_length, n_validation_files):\n",
    "    validation_data = get_validation_data(features_length, n_validation_files)\n",
    "    model.fit(\n",
    "        validation_data=validation_data,\n",
    "        x = training_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=10,\n",
    "        verbose=1)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    features_length = 1024\n",
    "    # we want it to run fast!\n",
    "    batch_size = 128\n",
    "    num_obs_per_epoch = 5000\n",
    "    \n",
    "    # create the model using the function from the model architecture section:\n",
    "    model = my_model(input_length = features_length)\n",
    "\n",
    "    # make the training data generator:\n",
    "    training_gen = make_training_gen(\n",
    "        batch_size = batch_size,\n",
    "        features_length=features_length)\n",
    "    \n",
    "    # and now train the model:\n",
    "    model.fit(\n",
    "        x = training_gen,\n",
    "        steps_per_epoch = num_obs_per_epoch / batch_size,\n",
    "        epochs=10,\n",
    "        verbose=1)\n",
    "\n",
    "    # now try getting some validation data:\n",
    "    validation_data = get_validation_data(features_length = features_length,\n",
    "                                            n_validation_files = 1000)\n",
    "    \n",
    "    # and train the model with training and validation data specified:\n",
    "    model.fit(\n",
    "        validation_data = validation_data,\n",
    "        x = training_gen,\n",
    "        steps_per_epoch = num_obs_per_epoch / batch_size,\n",
    "        epochs=10,\n",
    "        verbose=1)\n",
    "    # save the model\n",
    "    model.save('my_model.h5')\n",
    "    # load the model back into memory from the file:\n",
    "    same_model = load_model('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
